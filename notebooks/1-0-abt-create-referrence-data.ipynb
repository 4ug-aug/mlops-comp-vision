{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import cv2\n",
    "from src.data import utils\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_database(\n",
    "   now: str, features, prediction: int):\n",
    "   with open('../app/referrence_database.csv', 'a+') as file:\n",
    "      # Convert features to list\n",
    "      features = features.tolist()[0]\n",
    "      entry = f\"{now},\"\n",
    "      for feature in features:\n",
    "         entry += str(feature) + \", \"\n",
    "      entry += str(prediction) + \"\\n\"\n",
    "\n",
    "      if os.stat('../app/referrence_database.csv').st_size == 0:\n",
    "         # Write header\n",
    "         header = \"timestamp,\"\n",
    "         for i in range(1, len(features)):\n",
    "            header += f\"feature_{i}, \"\n",
    "         header += \"prediction\\n\"\n",
    "         file.write(header)\n",
    "         file.write(entry)\n",
    "      else:\n",
    "         file.write(entry)\n",
    "   return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_step(image_paths):\n",
    "   # Load model checkpoint\n",
    "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "   # Get latest model checkpoint from models/trained_models\n",
    "   latest_model_checkpoint = max([os.path.join(\"../models/trained_models\", f) for f in os.listdir(\"../models/trained_models\")], key=os.path.getctime)\n",
    "\n",
    "   model = torch.load(latest_model_checkpoint)\n",
    "\n",
    "   # Load image\n",
    "   image = Image.open(image_paths[0])\n",
    "   transform = transforms.ToTensor()\n",
    "   image = transform(image).unsqueeze(dim=0)\n",
    "   with torch.no_grad():\n",
    "      log_ps = model(image)\n",
    "\n",
    "      ps = torch.exp(log_ps)\n",
    "      top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "   return top_class.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over each image in data/raw/train\n",
    "data = pd.read_csv(\"../data/raw/BUTTERFLIES.csv\")\n",
    "trainset = data[data[\"data set\"] == \"train\"]\n",
    "h, w = 224, 224\n",
    "\n",
    "for i, row in tqdm(trainset.iterrows()):\n",
    "    img_path = f\"../data/raw/{row['filepaths']}\"\n",
    "\n",
    "    # Create metadata for image\n",
    "    now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "    # Load image as PIL\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    inputs = processor(text=None, images=img, return_tensors=\"pt\", padding=True)\n",
    "    img_features = model.get_image_features(inputs['pixel_values'])\n",
    "\n",
    "    # Predict\n",
    "    preds = predict_step([img_path])\n",
    "    add_to_database(now, img_features, preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f8a7f7f2e0a3a9dfabdf0866eabca43da6d8b6f7283b49e2fedbd5fa1d98f5e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
